\documentclass{article}

\usepackage{blindtext}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{geometry}
\geometry{
	a4paper,
	textwidth=15cm,
	textheight=23cm,
	top=2.5cm,
	bottom=2.5cm,
	left=2cm,
	right=2cm
}

\renewcommand{\v}[1]{\bm{#1}}
\newcommand{\E}{\mathbb{E}}

\begin{document}

\section{Hyperbolic random graphs}

Joint probability of the model:
\begin{equation}
\begin{split}
p(\v{A}, \v{r}, \v{\phi}, R,T,\alpha)
&= p(\v{r}, \v{\phi}, R,T,\alpha) p(\v{A} | \v{r}, \v{\phi}, R,T,\alpha)\\
&= p(\v{r}, \v{\phi} | R,T,\alpha) p(\v{A} | \v{r}, \v{\phi}, R,T,\alpha) p(R) p(T) p(\alpha)
\end{split}
\end{equation}

The joint probability of hyperbolic coordinates is given by
\begin{equation}
\begin{split}
p(\v{r}, \v{\phi} | R,T,\alpha) &= \frac{\alpha \sinh (\alpha \v{r})}{2 \pi (\cosh (\alpha R) -1)}\\
&= \prod_{i=1}^{N} \frac{\alpha \sinh (\alpha r_i)}{2 \pi (\cosh (\alpha R) -1)}
\end{split}
\end{equation}

The log probability of the edges is given by
\begin{equation}
\begin{split}
\log p(\v{A} | \v{r}, \v{\phi}, R,T,\alpha) 
&= \sum_{i,j} \biggl( A_{ij} \log \bigl(p(dist(i,j))\bigr) + (1-A_{ij}) \log \bigl(1 - p(dist(i,j))\bigr) \biggr)
\end{split}
\end{equation}

For each two nodes the probability of an edge depends on the hyperbolic distance:
\begin{equation}
\begin{split}
p \bigl(dist(u,v) \bigr)
&= \Biggl( 1 + \exp \biggl(\frac{1}{2T} \bigl(dist(u,v)-R \bigr)\biggr) \Biggr)^{-1}
\end{split}
\end{equation}

And the hyperbolic distance between nodes $i$ and $j$ is defined as
\begin{equation}
\begin{split}
dist(i,j) 
&= \cosh^{-1} \bigl( \cosh(r_i) \cosh(r_j) - \sinh(r_i) \sinh(r_j) \cos(\phi_i - \phi_j)\bigr)
\end{split}
\end{equation}

The variational distribution will be
\begin{equation}
\begin{split}
q(\v{r}, \v{\phi}, R,T,\alpha)
&= \prod_i q(r_i,\phi_i) q(R) q(T) q(\alpha)
\end{split}
\end{equation}

%\pagebreak
\section{ELBO}
\begin{equation}
\begin{aligned}
& \E_{q(\v{r}, \v{\phi}, R,T,\alpha)}[\log p(\v{A}, \v{r}, \v{\phi}, R,T,\alpha)] + H(q(\v{r}, \v{\phi}, R,T,\alpha)) \\ 
&= \E_{q( R,T,\alpha)} \Biggl[ 
\E_{q(\v{r}, \v{\phi})} \biggl[ \log p(\v{A} | \v{r}, \v{\phi}, R,T,\alpha) 
+ \log p(\v{r}, \v{\phi} | R,T,\alpha) +\log p(R) + \log p(T) + \log p(\alpha) \biggm\vert R,T,\alpha \biggr] \Biggr]\\ 
& \quad + H(q(\v{r}, \v{\phi}, R,T,\alpha)) \\
&= \E_{q( R,T,\alpha)} \Biggl[ 
\E_{q(\v{r}, \v{\phi})} \biggl[ 
\sum_{i,j} \biggl( A_{ij} \log \bigl(p(dist(i,j))\bigr) + (1-A_{ij}) \log \bigl(1 - p(dist(i,j))\bigr) \biggr) \biggm\vert R,T,\alpha \biggr]\\
& \quad+ \sum_i \E_{q(r_i,\phi_i)} \biggl[ \log(\sinh(\alpha r_i)) + \log(\alpha) - \log(2\pi) -\log(\cosh(\alpha R) -1) \biggm\vert R,T,\alpha \biggr] \Biggr]\\ 
& \quad +\E_{q(R)}[\log p(R)] + \E_{q(T)}[\log p(T)] + \E_{q(\alpha)}[\log p(\alpha)]\\
& \quad - \sum_i \E_{q(r_i,\phi_i)} [\log q(r_i,\phi_i)] - \E_{q(R)}[\log q(R)] - \E_{q(T)}[\log q(T)] - \E_{q(\alpha)}[\log q(\alpha)]  \\
\end{aligned}
\end{equation}

\section{ELBO for a minibatch}
\begin{equation}
\begin{split}
& \sum_{ij \atop i \neq j} \Biggl[ \frac{1}{N (N - 1)} \E_{q(R)}\biggl[\log p(R) - \log q(R)\biggr]  \\
& \quad + \frac{1}{N (N - 1)} \E_{q(T)}\biggl[\log p(T) - \log q(T)\biggr]  \\
& \quad + \frac{1}{N (N - 1)} \E_{q(\alpha)}\biggl[\log p(\alpha) - \log q(\alpha)\biggr]  \\
& \quad - \frac{1}{N-1} \Biggl( \E_{q(r_i)} \biggl[\log q(r_i) \biggr] + \E_{q(\phi_i)} \biggl[\log q(\phi_i) \biggr] \Biggr)\\
& \quad + \frac{1}{N-1} \Biggl( \E_{q(\alpha)}\bigl[\log (\alpha) \bigr] + \E_{q(R,\alpha)}\bigl[\log \bigl( \cosh (\alpha R) - 1\bigr) \bigr] - \log(2\pi) + \E_{q(r_i, \alpha)} \bigl[ \log(\sinh(\alpha r_i)) \bigr]\Biggr)\\
& \quad + \E_{q( R,T,\alpha)} \Biggl[ 
\E_{q(r_i, \phi_i)} \biggl[ 
A_{ij} \log \bigl(p(dist(i,j))\bigr) + (1-A_{ij}) \log \bigl(1 - p(dist(i,j))\bigr)  \biggm\vert R,T,\alpha \biggr] \Biggr] \Biggr]
\end{split}
\end{equation}

\subsection{Algorithm implementation}

Considering $R \geq 0$, $\alpha \geq 0$, $ 0 < T \leq 1$, $0 \leq r_i \leq R$ and $0 \leq \phi_i \leq 2\pi$, we assume that
\begin{align*}
\v{R} &\sim \mathtt{Gamma}(c_R, s_R)\\
\v{\alpha} &\sim \mathtt{Gamma}(c_\alpha, s_\alpha)\\
\v{T} &\sim \mathtt{Beta}(a_T, b_T)
\end{align*}
where $c_R$ and $c_\alpha$ are shapes (concentrations), $s_R$ and $s_\alpha$ are scales of $\mathtt{Gamma}$ distributions.\\

Further, we choose for $q(r_i)$ a $\mathtt{Radius}(\mu, \sigma, R)$ distribution, which is a $\mathtt{Normal}(\mu, \sigma^2)$ distribution mapped to a constrained space [0, 1] using \textit{sigmoid} function and then scaled on the interval $[0,R]$. This transformation in Pytorch is rather simple using the functionality of \verb|torch.distributions.transformed_distribution| and \verb|torch.distributions.transforms|. In this way the Jacobian correction is calculated automaticaly by the framework.\\

The choose of $q(\phi_i)$ is more complicated because it should be a spherical two-dimensional (in Cartesian coordinates) distribution. Ideally, we would take a wrapped normal but calculating its Jacobian troublesome, so we adapted a $\mathtt{von Mises-Fisher}$ distribution from \cite{vmf}. This implementation generally utilize rejection sampling for multi-dimensional distributions but for three-dimensional case a transformation of a uniform distribution is used. Encountering some problems with rejection sampling, which can be very slow, we chosen a three-dimensional variant projected on a two-dimensional plane.\\

\subsubsection*{Numerical stability}
For numerical stability we used \verb|log1mexp()| function defined in \cite{log1mexp}. This allows us to rewrite hyperbolic functions as
\begin{equation}
\begin{split}
\log \big(\sinh(\alpha r)\big) &= \log(1/2) - \log(e^{\alpha r}) + \log(e^{2 \alpha r}-1)\\
&= \log(1/2) - \alpha r + 2 \alpha r + \log(1 - e^{-2 \alpha r})\\
&= \log(1/2) + \alpha r + log1mexp (2 \alpha r)
\end{split}
\end{equation} 
and
\begin{equation}
\begin{split}
\log \big(\cosh(\alpha R) -1\big) &= \log(1/2) - \log(e^{\alpha R}) + 2 \log(e^{\alpha R}-1)\\
&= \log(1/2) - \alpha R + 2 \alpha R + \log(1 - e^{-\alpha R})\\
&= \log(1/2) + \alpha R + 2 log1mexp (\alpha R)
\end{split}
\end{equation} 
Then from (2) 
\begin{equation}
\begin{split}
\log \big( p(r_i, \phi_i | R,T,\alpha) \big) &= \log \Biggl( \frac{\alpha \sinh (\alpha r_i)}{2 \pi (\cosh (\alpha R) -1)} \Biggr)\\
&= \log( \alpha ) - \log(2\pi) + \alpha (r-R) + log1mexp (2 \alpha r) - 2\cdot log1mexp (\alpha R)
\end{split}
\end{equation}

\subsubsection*{Approximation for $\cosh^{-1}$}
The inverted $\cosh$ operation not only requires additional computation time but also cause some numerical instability.\\

Consider the hyperbolic distance
$$dist(i,j) = \cosh^{-1}(D)$$
where
$$ D(i,j) = \cosh(r_i) \cosh(r_j) - \sinh(r_i) \sinh(r_j) \cos(\phi_i - \phi_j)$$

Observe that for $y \gg 1$:
\begin{equation}
\begin{split}
\cosh^{-1}(y):= \log(y + \sqrt{y^2-1}) \approx \log(2y)
\end{split}
\end{equation}

That allows us to approximate the distance as $dist(i,j) \approx \log(2D)$. It will bound the true distance from below and will be quite precise for large distances. Observe however that if we plugin the approximation into the shifted sigmoid, appears in a region, where the sigmoid is near constant anyhow:

\begin{equation}
\begin{split}
p(dist(i,j))^{-1} &= 1 + \exp \bigg(\frac{\cosh^{-1}(D)}{2T}\bigg) \cdot \exp \bigg(\frac{-R}{2T}\bigg)\\
& \approx 1 + \big( 2D \big)^{1/(2T)}\cdot \exp \bigg(\frac{-R}{2T}\bigg)
\end{split}
\end{equation}

This solution is also not perfect. During the initialization or the further learning process $T$ can take a very small values, which can push the distance to an infinity and produce NaN values. The growth of $D$ can have the same consequences as well as $D=0$, that is why we have to clamp $D$. Moreover, clamping $p(dist(i,j))$ in the interval $(0,1)$ is also practical. 

\subsubsection*{The result optimization metric}

\begin{equation}
\begin{split}
ELBO_{HRG}(\Lambda) = &- \frac{L}{N^2} D_{KL} \left(q(R)||p(R)\right)\\
\quad &- \frac{L}{N^2} D_{KL} \left(q(T)||p(T)\right)\\
\quad &- \frac{L}{N^2} D_{KL} \left(q(\alpha)||p(\alpha)\right)\\
\quad & + \sum_{(i,j) \in \Lambda }\E_{q( R,T,\alpha)} \Biggl[ 
\E_{q(r_i, \phi_i)} \biggl[ 
A_{ij} \log \bigl(p(dist(i,j))\bigr) + (1-A_{ij}) \log \bigl(1 - p(dist(i,j))\bigr)  \biggm\vert R,T,\alpha \biggr] \Biggr]\\
\quad &+ \frac{L}{N^3} \sum_{(i,j) \in \Lambda} \E_{q(r_i, \alpha)} \bigl[ \alpha (r_i-R) + log1mexp (2 \alpha r_i) \bigr]\\
\quad &+ \frac{L}{N^2} \quad \E_{q(R, \alpha)} \bigl[ \log( \alpha ) - \log(2\pi) - 2\cdot log1mexp (\alpha R) \bigr]\\
\quad &- \frac{L}{N^3} \sum_{(i,j) \in \Lambda} \biggl(\E_{q(r_i)} \biggl[\log q(r_i) \biggr] + \E_{q(\phi_i)} \biggl[\log q(\phi_i) \biggr] \biggr) 
\end{split}
\end{equation} 

\begin{thebibliography}{}
	\bibitem{vmf}
	TR Davidson, L Falorsi, N De Cao, T Kipf, JM Tomczak. (2018). Hyperspherical variational Auto-Encoders. arXiv:1804.00891 \\
	\texttt{https://github.com/nicola-decao/s-vae-pytorch}
	
	\bibitem{log1mexp} 
	M\"achler, Martin. (2015). Accurately Computing $\log(1 - exp(-|a|))$ Assessed by the Rmpfr package. 10.13140/RG.2.2.11834.70084. 	
	
	
\end{thebibliography}

\end{document}
